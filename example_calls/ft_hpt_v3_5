export KMER=6
export MODEL_PATH=$PROJECT/models/pt_hpt/pt_hpt_v3_5
export DATA_PATH=$PROJECT/created_data/ft_hpt_v3_setup_v3
export OUTPUT_PATH=$PROJECT/models/ft_pt_hpt/ft_hpt_v3_5

python run_finetune.py \
    --model_type dna \
    --tokenizer_name=dna$KMER \
    --model_name_or_path $MODEL_PATH \
    --task_name dnaprom \
    --do_train \
    --do_eval \
    --data_dir $DATA_PATH \
    --max_seq_length 150 \
    --per_gpu_eval_batch_size=4096 \
    --per_gpu_train_batch_size=64 \
    --learning_rate 2e-5 \
    --num_train_epochs 2.0 \
    --output_dir $OUTPUT_PATH \
    --evaluate_during_training \
    --logging_steps 7500 \
    --save_steps 7500 \
    --warmup_percent 0.1 \
    --hidden_dropout_prob 0.3 \
    --overwrite_output \
    --weight_decay 0.01 \
    --n_process 24 \
    --fp16 \
    --freeze \
    --add_run_info 'ft run frozen layers. For hpt scale setup'